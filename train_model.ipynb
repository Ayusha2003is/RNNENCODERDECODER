{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1169,
   "id": "178fc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from numba import njit, float32, int32\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11300e",
   "metadata": {},
   "source": [
    "Loading the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "37a14532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (8000, 500), (8000, 50)\n",
      "Validation set: (1000, 500), (1000, 50)\n",
      "Test set: (1000, 500), (1000, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "train_src = np.load(\"../RNN_PREPROCESSED/train_src.npy\")\n",
    "train_tgt = np.load(\"../RNN_PREPROCESSED/train_tgt.npy\")\n",
    "val_src   = np.load(\"../RNN_PREPROCESSED/val_src.npy\")\n",
    "val_tgt   = np.load(\"../RNN_PREPROCESSED/val_tgt.npy\")\n",
    "\n",
    "# You want 10k total samples\n",
    "total_samples = 10000\n",
    "\n",
    "# Split sizes (80% train, 10% validation, 10% test)\n",
    "train_size = int(0.8 * total_samples)  # 8000 samples for training\n",
    "val_size = int(0.1 * total_samples)    # 1000 samples for validation\n",
    "test_size = total_samples - train_size - val_size  # 1000 samples for test\n",
    "\n",
    "# Shuffle the indices for train data only\n",
    "indices = np.random.permutation(train_src.shape[0])\n",
    "\n",
    "# Select 8000 samples for training\n",
    "train_indices = indices[:train_size]\n",
    "\n",
    "# Shuffle indices of val_src and val_tgt\n",
    "val_indices = np.random.permutation(val_src.shape[0])\n",
    "\n",
    "# Split val_indices into validation and test\n",
    "val_indices_split = val_indices[:val_size]  # First 1000 for validation\n",
    "test_indices_split = val_indices[val_size:val_size + test_size]  # Next 1000 for test\n",
    "\n",
    "# Reduce the datasets accordingly\n",
    "train_src = train_src[train_indices]\n",
    "train_tgt = train_tgt[train_indices]\n",
    "\n",
    "val_src_split = val_src[val_indices_split]\n",
    "val_tgt_split = val_tgt[val_indices_split]\n",
    "\n",
    "test_src = val_src[test_indices_split]\n",
    "test_tgt = val_tgt[test_indices_split]\n",
    "\n",
    "# Check the new shapes\n",
    "print(f\"Train set: {train_src.shape}, {train_tgt.shape}\")\n",
    "print(f\"Validation set: {val_src_split.shape}, {val_tgt_split.shape}\")\n",
    "print(f\"Test set: {test_src.shape}, {test_tgt.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17307992",
   "metadata": {},
   "source": [
    "encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "afb07048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clip_gradients_global(gradients, max_norm=5.0):\n",
    "    \"\"\"\n",
    "    gradients: list of numpy arrays (can be dW, db, etc.)\n",
    "    max_norm: maximum allowed global norm\n",
    "    \"\"\"\n",
    "    total_norm = 0.0\n",
    "    for grad in gradients:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    clip_coeff = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coeff < 1.0:\n",
    "        return [grad * clip_coeff for grad in gradients]\n",
    "    else:\n",
    "        return gradients  # no clipping needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "7b012c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True, fastmath=True)\n",
    "def softmax(logits):\n",
    "    # 1D softmax for a vector\n",
    "    max_logit = np.max(logits)\n",
    "    logits = logits - max_logit\n",
    "    exp_logits = np.exp(logits)\n",
    "    sum_exp = np.sum(exp_logits)\n",
    "    return exp_logits / sum_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6c0f5",
   "metadata": {},
   "source": [
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "34fa0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_dropout(x, dropout_prob, training=True):\n",
    "    \"\"\"\n",
    "    Applies inverted dropout to the input tensor.\n",
    "\n",
    "    Args:\n",
    "        x: np.ndarray, input tensor of any shape\n",
    "        dropout_prob: float in [0, 1), probability of dropping a unit\n",
    "        training: bool, whether to apply dropout or not\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray, same shape as x, with dropout applied if training=True\n",
    "    \"\"\"\n",
    "    if not training or dropout_prob <= 0.0:\n",
    "        return x\n",
    "\n",
    "    keep_prob = 1.0 - dropout_prob\n",
    "    mask = (np.random.rand(*x.shape) < keep_prob).astype(np.float32)\n",
    "    return (x * mask) / keep_prob  # inverted scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "a50b646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@njit\n",
    "def tanh_grad(h):\n",
    "    return 1 - h**2\n",
    "\n",
    "@njit\n",
    "def concat_dot_tanh(W, h_prev, x_t, b):\n",
    "    concat = np.vstack((h_prev, x_t))\n",
    "    z = np.dot(W, concat) + b\n",
    "    h_next = np.tanh(z)\n",
    "    return h_next, concat, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "7d8832f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_token=0, max_len=None):\n",
    "    \"\"\"\n",
    "    Pads a list of sequences to the same length (max_len).\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)  # Or set max_len manually if needed\n",
    "\n",
    "    batch_size = len(sequences)\n",
    "    padded = np.full((batch_size, max_len), pad_token, dtype=np.int32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_len)  # Ensure no sequence exceeds max_len\n",
    "        padded[i, :length] = seq[:length]  # Pad with 0 or your pad_token\n",
    "\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "c07ed062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_sorted_batches(src_seqs, tgt_seqs, batch_size=64, pad_token=0):\n",
    "    \"\"\"\n",
    "    Create batches with proper padding to handle different sequence lengths.\n",
    "    \"\"\"\n",
    "    # Pair sequences with their lengths for sorting\n",
    "    paired_data = list(zip(src_seqs, tgt_seqs))\n",
    "    \n",
    "    # Sort by source sequence length to minimize padding within batches\n",
    "    paired_data.sort(key=lambda x: len(x[0]))\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(paired_data), batch_size):\n",
    "        batch_pairs = paired_data[i:i + batch_size]\n",
    "        \n",
    "        # Separate source and target sequences\n",
    "        src_batch = [pair[0] for pair in batch_pairs]\n",
    "        tgt_batch = [pair[1] for pair in batch_pairs]\n",
    "        \n",
    "        # Pad sequences to the maximum length in this batch\n",
    "        max_src_len = max(len(seq) for seq in src_batch)\n",
    "        max_tgt_len = max(len(seq) for seq in tgt_batch)\n",
    "        \n",
    "        # Pad source sequences\n",
    "        padded_src = []\n",
    "        for seq in src_batch:\n",
    "            padded_seq = np.pad(seq, (0, max_src_len - len(seq)), \n",
    "                               mode='constant', constant_values=pad_token)\n",
    "            padded_src.append(padded_seq)\n",
    "        \n",
    "        # Pad target sequences\n",
    "        padded_tgt = []\n",
    "        for seq in tgt_batch:\n",
    "            padded_seq = np.pad(seq, (0, max_tgt_len - len(seq)), \n",
    "                               mode='constant', constant_values=pad_token)\n",
    "            padded_tgt.append(padded_seq)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        src_batch_array = np.array(padded_src)\n",
    "        tgt_batch_array = np.array(padded_tgt)\n",
    "        \n",
    "        batches.append((src_batch_array, tgt_batch_array))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "e29a04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def masked_cross_entropy_loss_optimized(logits, targets, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Optimized masked cross-entropy loss for batch processing.\n",
    "\n",
    "    Args:\n",
    "        logits: [batch_size, seq_len, vocab_size] - model predictions\n",
    "        targets: [batch_size, seq_len] - ground truth token IDs\n",
    "        pad_idx: int - padding token ID to ignore in loss calculation\n",
    "\n",
    "    Returns:\n",
    "        loss: float - average loss per non-padding token\n",
    "        grad: [batch_size, seq_len, vocab_size] - gradients w.r.t. logits\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "    # Flatten for easier processing\n",
    "    logits_flat = logits.reshape(-1, vocab_size)           # [B * T, V]\n",
    "    targets_flat = targets.reshape(-1)                     # [B * T]\n",
    "    mask = (targets_flat != pad_idx).astype(np.float32)    # [B * T]\n",
    "\n",
    "    # Logits stabilization\n",
    "    logits_stable = logits_flat - np.max(logits_flat, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_stable)\n",
    "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    # Gather probabilities of correct classes\n",
    "    correct_probs = probs[np.arange(len(targets_flat)), targets_flat]\n",
    "    log_probs = np.log(correct_probs + 1e-12)\n",
    "\n",
    "    # Compute masked loss\n",
    "    masked_log_probs = log_probs * mask\n",
    "    total_loss = -np.sum(masked_log_probs)\n",
    "    total_tokens = np.sum(mask)\n",
    "    loss = total_loss / max(total_tokens, 1)\n",
    "\n",
    "    # Compute gradients\n",
    "    grad_flat = probs.copy()\n",
    "    grad_flat[np.arange(len(targets_flat)), targets_flat] -= 1\n",
    "    grad_flat *= mask[:, np.newaxis]\n",
    "\n",
    "    # Reshape gradients to [B, T, V]\n",
    "    grad = grad_flat.reshape(batch_size, seq_len, vocab_size)\n",
    "\n",
    "    return loss, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "57cb46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss_single_step(logits, targets, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Compute masked cross-entropy loss for a single timestep.\n",
    "    logits: [batch_size, vocab_size]\n",
    "    targets: [batch_size]\n",
    "    \"\"\"\n",
    "    mask = (targets != pad_idx).astype(np.float32)\n",
    "\n",
    "    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    probs = np.exp(logits_stable)\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "    correct_probs = probs[np.arange(len(targets)), targets]\n",
    "    log_probs = np.log(correct_probs + 1e-12)\n",
    "\n",
    "    masked_log_probs = log_probs * mask\n",
    "    loss = -np.sum(masked_log_probs) / max(np.sum(mask), 1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "804c8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss(outputs, targets, pad_token_id=0):\n",
    "    \"\"\"\n",
    "    Compute sequence loss over all timesteps.\n",
    "\n",
    "    Args:\n",
    "        outputs: [seq_len, batch_size, vocab_size] - model predictions for each timestep\n",
    "        targets: [seq_len, batch_size] - ground truth targets\n",
    "        pad_token_id: int - padding token ID\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: float - average loss per token\n",
    "        gradients: [seq_len, batch_size, vocab_size] - gradients w.r.t. outputs\n",
    "    \"\"\"\n",
    "    seq_len, batch_size, vocab_size = outputs.shape\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    gradients = np.zeros_like(outputs)\n",
    "\n",
    "    for t in range(1, seq_len):  # skip <sos>\n",
    "        logits = outputs[t]               # [B, V]\n",
    "        target_tokens = targets[t]        # [B]\n",
    "        mask = (target_tokens != pad_token_id).astype(np.float32)\n",
    "\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n",
    "        exp_logits = np.exp(logits_stable)\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "        correct_probs = probs[np.arange(batch_size), target_tokens]\n",
    "        log_probs = np.log(correct_probs + 1e-12)\n",
    "        masked_losses = -log_probs * mask\n",
    "\n",
    "        timestep_loss = np.sum(masked_losses)\n",
    "        timestep_tokens = np.sum(mask)\n",
    "\n",
    "        total_loss += timestep_loss\n",
    "        total_tokens += timestep_tokens\n",
    "\n",
    "        grad = probs.copy()\n",
    "        grad[np.arange(batch_size), target_tokens] -= 1\n",
    "        grad *= mask[:, np.newaxis]\n",
    "\n",
    "        gradients[t] = grad\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    return avg_loss, gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "139be40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EfficientDataLoader:\n",
    "    \"\"\"\n",
    "    Efficient data loader for sequence-to-sequence training.\n",
    "    Handles batching, padding, and shuffling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_data, tgt_data, batch_size=32, pad_token=0):\n",
    "        \"\"\"\n",
    "        Initialize the data loader.\n",
    "        \n",
    "        Args:\n",
    "            src_data: list of source sequences\n",
    "            tgt_data: list of target sequences\n",
    "            batch_size: int - batch size\n",
    "            pad_token: int - padding token ID\n",
    "        \"\"\"\n",
    "        assert len(src_data) == len(tgt_data), \"Source and target data must have same length\"\n",
    "        \n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_token = pad_token\n",
    "        self.n_samples = len(src_data)\n",
    "        \n",
    "        # Initialize indices\n",
    "        self.indices = np.arange(self.n_samples)\n",
    "        \n",
    "        # Compute number of batches\n",
    "        self.n_batches = (self.n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffle the data indices for randomized batching.\"\"\"\n",
    "        np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of batches.\"\"\"\n",
    "        return self.n_batches\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield padded batches as [seq_len, batch_size] numpy arrays.\"\"\"\n",
    "        for i in range(0, self.n_samples, self.batch_size):\n",
    "            # Get batch indices\n",
    "            batch_indices = self.indices[i:i + self.batch_size]\n",
    "            \n",
    "            # Get sequences for this batch\n",
    "            batch_src = [self.src_data[idx] for idx in batch_indices]\n",
    "            batch_tgt = [self.tgt_data[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Find max sequence lengths\n",
    "            max_src_len = max(len(seq) for seq in batch_src)\n",
    "            max_tgt_len = max(len(seq) for seq in batch_tgt)\n",
    "            \n",
    "            # Initialize padded arrays\n",
    "            padded_src = np.full((max_src_len, len(batch_src)), self.pad_token, dtype=np.int32)\n",
    "            padded_tgt = np.full((max_tgt_len, len(batch_tgt)), self.pad_token, dtype=np.int32)\n",
    "            \n",
    "            # Fill with actual sequences\n",
    "            for j, seq in enumerate(batch_src):\n",
    "                if len(seq) > 0:\n",
    "                    padded_src[:len(seq), j] = seq\n",
    "            for j, seq in enumerate(batch_tgt):\n",
    "                if len(seq) > 0:\n",
    "                    padded_tgt[:len(seq), j] = seq\n",
    "            \n",
    "            yield padded_src, padded_tgt\n",
    "    \n",
    "    def get_batch_stats(self):\n",
    "        \"\"\"Return a dictionary with basic data stats.\"\"\"\n",
    "        src_lengths = [len(seq) for seq in self.src_data]\n",
    "        tgt_lengths = [len(seq) for seq in self.tgt_data]\n",
    "        \n",
    "        return {\n",
    "            'n_samples': self.n_samples,\n",
    "            'n_batches': self.n_batches,\n",
    "            'avg_src_len': np.mean(src_lengths),\n",
    "            'max_src_len': np.max(src_lengths),\n",
    "            'avg_tgt_len': np.mean(tgt_lengths),\n",
    "            'max_tgt_len': np.max(tgt_lengths),\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "    \n",
    "    def sort_by_length(self, by_source=True):\n",
    "        \"\"\"\n",
    "        Sort sequences by length to improve efficiency of batching.\n",
    "        \n",
    "        Args:\n",
    "            by_source: bool - sort by source (True) or target (False) length\n",
    "        \"\"\"\n",
    "        lengths = [len(seq) for seq in (self.src_data if by_source else self.tgt_data)]\n",
    "        sorted_indices = np.argsort(lengths)\n",
    "        self.indices = sorted_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "542f012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_batches(\n",
    "    src_data, tgt_data,\n",
    "    batch_size=32,\n",
    "    pad_token=0,\n",
    "    drop_last=False,\n",
    "    src_max_len=None,\n",
    "    tgt_max_len=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Yields batches of (src_batch, tgt_batch)\n",
    "    Each batch is padded and transposed: [seq_len, batch_size]\n",
    "\n",
    "    Args:\n",
    "        src_data: list of source sequences (lists of ints)\n",
    "        tgt_data: list of target sequences (lists of ints)\n",
    "        batch_size: number of samples per batch\n",
    "        pad_token: token used for padding\n",
    "        drop_last: if True, drops last batch if incomplete\n",
    "        src_max_len: truncate/pad src to this length (None = max in batch)\n",
    "        tgt_max_len: same for tgt\n",
    "    \"\"\"\n",
    "    assert len(src_data) == len(tgt_data), \"Source & target data must be same length\"\n",
    "\n",
    "    indices = np.arange(len(src_data))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    total_samples = len(indices)\n",
    "    num_batches = total_samples // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_indices = indices[i * batch_size : (i + 1) * batch_size]\n",
    "        batch_src = [src_data[idx] for idx in batch_indices]\n",
    "        batch_tgt = [tgt_data[idx] for idx in batch_indices]\n",
    "\n",
    "        src_padded = pad_sequences(batch_src, pad_token, max_len=src_max_len)\n",
    "        tgt_padded = pad_sequences(batch_tgt, pad_token, max_len=tgt_max_len)\n",
    "\n",
    "        yield src_padded.T, tgt_padded.T\n",
    "\n",
    "    if not drop_last and (total_samples % batch_size != 0):\n",
    "        batch_indices = indices[num_batches * batch_size :]\n",
    "        batch_src = [src_data[idx] for idx in batch_indices]\n",
    "        batch_tgt = [tgt_data[idx] for idx in batch_indices]\n",
    "\n",
    "        src_padded = pad_sequences(batch_src, pad_token, max_len=src_max_len)\n",
    "        tgt_padded = pad_sequences(batch_tgt, pad_token, max_len=tgt_max_len)\n",
    "\n",
    "        yield src_padded.T, tgt_padded.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "e4c9ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder:\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout_prob=0.0):\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.embedding = (np.random.randn(input_dim, embedding_dim) * 0.01).astype(np.float32)\n",
    "        self.rnn_params = []\n",
    "        input_dim_now = embedding_dim\n",
    "        for _ in range(n_layers):\n",
    "            W = (np.random.randn(hidden_dim, hidden_dim + input_dim_now) * 0.01).astype(np.float32)\n",
    "            b = np.zeros((hidden_dim, 1), dtype=np.float32)\n",
    "            self.rnn_params.append((W, b))\n",
    "            input_dim_now = hidden_dim\n",
    "\n",
    "        self.saved_states = []\n",
    "\n",
    "    def rnn_cell_forward(self, x_t, h_prev, W, b):\n",
    "        concat = np.vstack((h_prev, x_t))\n",
    "        z = np.dot(W, concat) + b\n",
    "        h_next = np.tanh(z)\n",
    "        return h_next, concat, z\n",
    "\n",
    "    def forward(self, src, training=True):\n",
    "        seq_len, batch_size = src.shape\n",
    "        embedded = np.take(self.embedding, src, axis=0).transpose(0, 2, 1)\n",
    "\n",
    "        h = np.zeros((self.n_layers, self.hidden_dim, batch_size), dtype=np.float32)\n",
    "        self.saved_states = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            timestep_states = []\n",
    "            x = embedded[t]\n",
    "            x = apply_dropout(x, self.dropout_prob, training)\n",
    "\n",
    "            for l in range(self.n_layers):\n",
    "                W, b = self.rnn_params[l]\n",
    "                h_prev = h[l]\n",
    "                h_next, concat, z = self.rnn_cell_forward(x, h_prev, W, b)\n",
    "                h_next = apply_dropout(h_next, self.dropout_prob, training)\n",
    "                h[l] = h_next\n",
    "                x = h_next\n",
    "                timestep_states.append((h_prev, concat, h_next, z))\n",
    "\n",
    "            self.saved_states.append(timestep_states)\n",
    "\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad_on_encoder_outputs, learning_rate=0.01):\n",
    "        seq_len = len(self.saved_states)\n",
    "        batch_size = grad_on_encoder_outputs.shape[2]\n",
    "\n",
    "        dW_total = [np.zeros_like(W) for W, _ in self.rnn_params]\n",
    "        db_total = [np.zeros_like(b) for _, b in self.rnn_params]\n",
    "        dh_next_layers = [np.zeros((self.hidden_dim, batch_size), dtype=np.float32) for _ in range(self.n_layers)]\n",
    "        d_embedding_total = np.zeros_like(self.embedding)\n",
    "\n",
    "        for t in reversed(range(seq_len)):\n",
    "            timestep_states = self.saved_states[t]\n",
    "\n",
    "            if grad_on_encoder_outputs is not None:\n",
    "                dh_next_layers[self.n_layers - 1] += grad_on_encoder_outputs[t]\n",
    "\n",
    "            for l in reversed(range(self.n_layers)):\n",
    "                W, b = self.rnn_params[l]\n",
    "                h_prev, concat, h_next, z = timestep_states[l]\n",
    "\n",
    "                dz = (1 - h_next**2) * dh_next_layers[l]\n",
    "                dW = np.dot(dz, concat.T)\n",
    "                db = np.sum(dz, axis=1, keepdims=True)\n",
    "                dW_total[l] += dW\n",
    "                db_total[l] += db\n",
    "\n",
    "                d_concat = np.dot(W.T, dz)\n",
    "                dh_prev = d_concat[:self.hidden_dim, :]\n",
    "                dx = d_concat[self.hidden_dim:, :]\n",
    "\n",
    "                dh_next_layers[l] = dh_prev\n",
    "\n",
    "                if l > 0:\n",
    "                    dh_next_layers[l - 1] += dx\n",
    "\n",
    "        all_grads = dW_total + db_total\n",
    "        all_grads.append(d_embedding_total)\n",
    "        clipped_grads = clip_gradients_global(all_grads)\n",
    "\n",
    "        n_rnn_params = len(self.rnn_params)\n",
    "        dW_total_clipped = clipped_grads[:n_rnn_params]\n",
    "        db_total_clipped = clipped_grads[n_rnn_params:2 * n_rnn_params]\n",
    "        d_embedding_total_clipped = clipped_grads[-1]\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            W, b = self.rnn_params[l]\n",
    "            self.rnn_params[l] = (W - learning_rate * dW_total_clipped[l],\n",
    "                                  b - learning_rate * db_total_clipped[l])\n",
    "\n",
    "        self.embedding -= learning_rate * d_embedding_total_clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "9739f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DotProductAttention:\n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch, hidden_dim]\n",
    "        keys: [batch, seq_len, hidden_dim]\n",
    "        values: [batch, seq_len, hidden_dim]\n",
    "        mask: [batch, seq_len] or None\n",
    "        \"\"\"\n",
    "        scores = np.einsum('bh,bsh->bs', query, keys)  # [batch, seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)\n",
    "\n",
    "        scores_max = np.max(scores, axis=1, keepdims=True)\n",
    "        scores_exp = np.exp(scores - scores_max)\n",
    "        attn_weights = scores_exp / np.sum(scores_exp, axis=1, keepdims=True)\n",
    "\n",
    "        context = np.einsum('bs,bsh->bh', attn_weights, values)\n",
    "\n",
    "        self.saved = (query, keys, values, attn_weights, mask)\n",
    "        return context, attn_weights\n",
    "\n",
    "    def backward(self, d_context):\n",
    "        \"\"\"\n",
    "        d_context: [batch, hidden_dim]\n",
    "        Returns:\n",
    "            d_query: [batch, hidden_dim]\n",
    "            d_keys: [batch, seq_len, hidden_dim]\n",
    "            d_values: [batch, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        query, keys, values, attn_weights, mask = self.saved\n",
    "        batch_size, seq_len, hidden_dim = keys.shape\n",
    "\n",
    "        # dL/d(attn_weights)\n",
    "        d_attn_weights = np.einsum('bh,bsh->bs', d_context, values)  # [batch, seq_len]\n",
    "\n",
    "        # dL/d(values)\n",
    "        d_values = attn_weights[:, :, None] * d_context[:, None, :]  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Softmax gradient: Jacobian-vector product\n",
    "        sum_term = np.sum(d_attn_weights * attn_weights, axis=1, keepdims=True)  # [batch, 1]\n",
    "        d_scores = attn_weights * (d_attn_weights - sum_term)  # [batch, seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            d_scores = np.where(mask == 0, 0, d_scores)\n",
    "\n",
    "        # dL/d(query)\n",
    "        d_query = np.einsum('bs,bsh->bh', d_scores, keys)  # [batch, hidden_dim]\n",
    "\n",
    "        # dL/d(keys)\n",
    "        d_keys = np.einsum('bs,bh->bsh', d_scores, query)  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "        return d_query, d_keys, d_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "c51e0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNNDecoder:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_prob=0.0):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Embedding + RNN Params\n",
    "        self.embedding = (np.random.randn(vocab_size, embedding_dim) * 0.01).astype(np.float32)\n",
    "        self.rnn_params = []\n",
    "        input_dim_now = embedding_dim + hidden_dim  # concat with attention context\n",
    "        for _ in range(n_layers):\n",
    "            W = (np.random.randn(hidden_dim, hidden_dim + input_dim_now) * 0.01).astype(np.float32)\n",
    "            b = np.zeros((hidden_dim, 1), dtype=np.float32)\n",
    "            self.rnn_params.append((W, b))\n",
    "            input_dim_now = hidden_dim\n",
    "\n",
    "        # Output projection\n",
    "        self.fc_W = (np.random.randn(vocab_size, hidden_dim) * 0.001).astype(np.float32)\n",
    "        self.fc_b = np.zeros((vocab_size, 1), dtype=np.float32)\n",
    "\n",
    "        self.attention = DotProductAttention()\n",
    "\n",
    "        # Saved states\n",
    "        self.saved_hiddens = []\n",
    "        self.attention_scores = []\n",
    "        self.saved_input_tokens = []\n",
    "\n",
    "    def rnn_cell_forward(self, x, h_prev, W, b):\n",
    "        concat = np.vstack((h_prev, x))\n",
    "        z = np.dot(W, concat) + b\n",
    "        h_next = np.tanh(z)\n",
    "        return h_next, concat, z\n",
    "\n",
    "    def forward(self, input_tokens, hidden, encoder_outputs, training=True):\n",
    "        batch_size = input_tokens.shape[0]\n",
    "\n",
    "        if training:\n",
    "            self.saved_input_tokens.append(input_tokens.copy())\n",
    "            self.saved_hiddens.append([])\n",
    "            self.attention_scores.append([])\n",
    "\n",
    "        input_emb = self.embedding[input_tokens].T\n",
    "        input_emb = apply_dropout(input_emb, self.dropout_prob, training)\n",
    "\n",
    "        query = hidden[-1].T\n",
    "        context, attn_weights = self.attention.forward(query, encoder_outputs, encoder_outputs)\n",
    "        context_T = context.T\n",
    "\n",
    "        x = np.vstack((input_emb, context_T))\n",
    "\n",
    "        new_hidden = np.zeros_like(hidden)\n",
    "        timestep_hiddens = []\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            W, b = self.rnn_params[l]\n",
    "            h_prev = hidden[l]\n",
    "            h_next, concat, z = self.rnn_cell_forward(x, h_prev, W, b)\n",
    "            h_next = apply_dropout(h_next, self.dropout_prob, training)\n",
    "            new_hidden[l] = h_next\n",
    "            x = h_next\n",
    "            timestep_hiddens.append((h_prev, concat, h_next, z))\n",
    "\n",
    "        if training:\n",
    "            self.saved_hiddens[-1] = timestep_hiddens\n",
    "            self.attention_scores[-1] = attn_weights\n",
    "\n",
    "        logits = np.dot(self.fc_W, h_next) + self.fc_b\n",
    "        return logits.T, new_hidden  # (batch, vocab_size), new_hidden\n",
    "\n",
    "    def backward(self, grad_outputs, hidden_states, encoder_outputs, learning_rate=0.01):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        seq_len, hidden_dim = encoder_outputs.shape[1], encoder_outputs.shape[2]\n",
    "\n",
    "        dW_total = [np.zeros_like(W) for W, _ in self.rnn_params]\n",
    "        db_total = [np.zeros_like(b) for _, b in self.rnn_params]\n",
    "        d_fc_W = np.zeros_like(self.fc_W)\n",
    "        d_fc_b = np.zeros_like(self.fc_b)\n",
    "        d_encoder = np.zeros((seq_len, hidden_dim, batch_size), dtype=np.float32)\n",
    "        dh_next = [np.zeros((self.hidden_dim, batch_size), dtype=np.float32) for _ in range(self.n_layers)]\n",
    "        d_embedding = np.zeros_like(self.embedding)\n",
    "\n",
    "        for t in reversed(range(len(grad_outputs))):\n",
    "            grad_output = grad_outputs[t]\n",
    "            hidden = hidden_states[t]\n",
    "            h_last = hidden[-1]\n",
    "\n",
    "            # FC backward\n",
    "            d_fc_W += np.dot(grad_output.T, h_last.T)\n",
    "            d_fc_b += np.sum(grad_output.T, axis=1, keepdims=True)\n",
    "            dh = np.dot(self.fc_W.T, grad_output.T)\n",
    "\n",
    "            for l in reversed(range(self.n_layers)):\n",
    "                W, b = self.rnn_params[l]\n",
    "                h_prev, concat, h_next, z = self.saved_hiddens[t][l]\n",
    "\n",
    "                dz = (1 - h_next ** 2) * (dh + dh_next[l])  # tanh' * total gradient\n",
    "                dW = np.dot(dz, concat.T)\n",
    "                db = np.sum(dz, axis=1, keepdims=True)\n",
    "\n",
    "                dW_total[l] += dW\n",
    "                db_total[l] += db\n",
    "\n",
    "                d_concat = np.dot(W.T, dz)\n",
    "                dh = d_concat[:self.hidden_dim, :]\n",
    "                dx = d_concat[self.hidden_dim:, :]\n",
    "\n",
    "                dh_next[l] += dh  # ACCUMULATE gradients here ‚úÖ\n",
    "\n",
    "                if l == 0:\n",
    "                    d_input_emb = dx[:self.embedding_dim, :]\n",
    "                    d_context = dx[self.embedding_dim:, :]\n",
    "\n",
    "                    d_query, d_keys, d_values = self.attention.backward(d_context.T)\n",
    "                    dh_next[self.n_layers - 1] += d_query.T\n",
    "                    d_encoder += d_keys.transpose(1, 0, 2).transpose(0, 2, 1) + d_values.transpose(1, 0, 2).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "\n",
    "                    input_tokens = self.saved_input_tokens[t]\n",
    "                    for i, token in enumerate(input_tokens):\n",
    "                        d_embedding[token] += d_input_emb[:, i]\n",
    "\n",
    "        # === Apply Gradients ===\n",
    "        all_grads = dW_total + db_total + [d_fc_W, d_fc_b, d_embedding]\n",
    "        clipped_grads = clip_gradients_global(all_grads)\n",
    "\n",
    "        dW_total = clipped_grads[:self.n_layers]\n",
    "        db_total = clipped_grads[self.n_layers:2 * self.n_layers]\n",
    "        d_fc_W = clipped_grads[2 * self.n_layers]\n",
    "        d_fc_b = clipped_grads[2 * self.n_layers + 1]\n",
    "        d_embedding = clipped_grads[2 * self.n_layers + 2]\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            W, b = self.rnn_params[l]\n",
    "            self.rnn_params[l] = (\n",
    "                W - learning_rate * dW_total[l],\n",
    "                b - learning_rate * db_total[l]\n",
    "            )\n",
    "\n",
    "        self.fc_W -= learning_rate * d_fc_W\n",
    "        self.fc_b -= learning_rate * d_fc_b\n",
    "        self.embedding -= learning_rate * d_embedding\n",
    "\n",
    "        return d_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "fdf17083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5, training=True):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "        outputs = np.zeros((trg_len, batch_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "        # Get encoder outputs and final hidden state\n",
    "        hidden = self.encoder.forward(src, training=training)\n",
    "        \n",
    "        encoder_states = []\n",
    "        for layer_states in self.encoder.saved_states:\n",
    "            # Get h_next from last layer\n",
    "            h_next = layer_states[-1][2]  # Shape: (hidden_dim, batch_size)\n",
    "            encoder_states.append(h_next)\n",
    "        \n",
    "        # Stack to get (seq_len, hidden_dim, batch_size)\n",
    "        encoder_outputs = np.stack(encoder_states, axis=0)\n",
    "        \n",
    "        # Transpose to (batch_size, seq_len, hidden_dim)\n",
    "        encoder_outputs = encoder_outputs.transpose(2, 0, 1)\n",
    "        \n",
    "        self.encoder_outputs = encoder_outputs\n",
    "\n",
    "        input_token = trg[0, :]  # First token is <sos>\n",
    "        \n",
    "        # Initialize decoder state tracking\n",
    "        self.decoder.saved_hiddens = []\n",
    "        self.decoder.attention_scores = []\n",
    "        self.decoder_states = []\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Forward pass through decoder with attention\n",
    "            output, hidden = self.decoder.forward(\n",
    "                input_token, hidden, encoder_outputs, training=training\n",
    "            )\n",
    "            \n",
    "            outputs[t] = output\n",
    "            self.decoder_states.append(hidden)\n",
    "            \n",
    "            # Teacher forcing logic\n",
    "            teacher_force = random.random() < teacher_forcing_ratio if training else False\n",
    "            top1 = np.argmax(output, axis=1)\n",
    "            input_token = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1fc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(seq2seq, src_batch, tgt_batch, learning_rate=0.01,\n",
    "               teacher_forcing_ratio=1.0, pad_idx=0, chunk_size=10):\n",
    "    \"\"\"\n",
    "    Memory-efficient training step with chunked loss and debug softmax print.\n",
    "\n",
    "    Args:\n",
    "        seq2seq: Seq2Seq model\n",
    "        src_batch: [src_len, batch_size]\n",
    "        tgt_batch: [tgt_len, batch_size]\n",
    "        learning_rate: float\n",
    "        teacher_forcing_ratio: float\n",
    "        pad_idx: int\n",
    "        chunk_size: int\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: float\n",
    "    \"\"\"\n",
    "    # Clear state\n",
    "    seq2seq.encoder.saved_states = []\n",
    "    seq2seq.decoder.saved_hiddens = []\n",
    "    seq2seq.decoder.attention_scores = []\n",
    "\n",
    "    # === Forward ===\n",
    "    outputs = seq2seq.forward(\n",
    "        src_batch, tgt_batch,\n",
    "        teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "        training=True\n",
    "    )\n",
    "\n",
    "    # === Debug: Softmax at timestep 1, batch 0 ===\n",
    "    if outputs.shape[0] > 1 and outputs.shape[1] > 0:\n",
    "        logits_sample = outputs[1][0]  # (vocab,)\n",
    "        probs = np.exp(logits_sample) / np.sum(np.exp(logits_sample))\n",
    "        print(\"üîç Softmax Probs Sample (step=1, batch=0):\", probs[:10])\n",
    "        print(\"üî¢ Sum of probs:\", np.sum(probs))  # should be ~1.0\n",
    "\n",
    "    trg_len, batch_size = tgt_batch.shape\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # === Chunked training ===\n",
    "    for chunk_start in range(1, trg_len, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, trg_len)\n",
    "\n",
    "        # Get chunk\n",
    "        logits_chunk = outputs[chunk_start:chunk_end]      # [chunk, batch, vocab]\n",
    "        targets_chunk = tgt_batch[chunk_start:chunk_end]   # [chunk, batch]\n",
    "\n",
    "        logits_reshaped = logits_chunk.transpose(1, 0, 2)  # [B, T, V]\n",
    "        targets_reshaped = targets_chunk.T                 # [B, T]\n",
    "\n",
    "        # Loss & gradient\n",
    "        loss, grad = masked_cross_entropy_loss_optimized(logits_reshaped, targets_reshaped, pad_idx)\n",
    "\n",
    "        # Count non-pad tokens\n",
    "        mask = (targets_reshaped != pad_idx).astype(np.float32)\n",
    "        chunk_tokens = np.sum(mask)\n",
    "        total_loss += loss * chunk_tokens\n",
    "        total_tokens += chunk_tokens\n",
    "\n",
    "        # Gradients per timestep\n",
    "        grad_outputs = [grad[:, t, :] for t in range(grad.shape[1])]\n",
    "\n",
    "        # Backward\n",
    "        decoder_grads = seq2seq.decoder.backward(\n",
    "            grad_outputs,\n",
    "            seq2seq.decoder_states[chunk_start - 1:chunk_end - 1],\n",
    "            seq2seq.encoder_outputs,\n",
    "            learning_rate\n",
    "        )\n",
    "        seq2seq.encoder.backward(decoder_grads, learning_rate)\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "57d52ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_model(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    path=\"model_checkpoint.npz\",\n",
    "    fallback_lr=0.005\n",
    "):\n",
    "    \"\"\"\n",
    "    Load encoder and decoder weights + optional metadata.\n",
    "    Handles missing keys gracefully.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {path}\")\n",
    "\n",
    "    with np.load(path, allow_pickle=True) as data:\n",
    "        try:\n",
    "            encoder.embedding[:] = data[\"encoder_embedding\"]\n",
    "            decoder.embedding[:] = data[\"decoder_embedding\"]\n",
    "\n",
    "            decoder.fc_W[:] = data[\"decoder_fc_W\"]\n",
    "            decoder.fc_b[:] = data[\"decoder_fc_b\"]\n",
    "\n",
    "            for i in range(encoder.n_layers):\n",
    "                W, b = encoder.rnn_params[i]\n",
    "                W[:] = data[f\"encoder_W_{i}\"]\n",
    "                b[:] = data[f\"encoder_b_{i}\"]\n",
    "\n",
    "            for i in range(decoder.n_layers):\n",
    "                W, b = decoder.rnn_params[i]\n",
    "                W[:] = data[f\"decoder_W_{i}\"]\n",
    "                b[:] = data[f\"decoder_b_{i}\"]\n",
    "\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Missing key in checkpoint: {e}\")\n",
    "\n",
    "        print(f\"‚úÖ Model loaded from '{path}'\")\n",
    "\n",
    "        # Optional metadata \n",
    "        train_losses = data.get(\"train_losses\", [])\n",
    "        val_losses = data.get(\"val_losses\", [])\n",
    "        epoch = int(data.get(\"epoch\", 0))\n",
    "        learning_rate = float(data.get(\"learning_rate\", fallback_lr))\n",
    "\n",
    "    train_losses = train_losses.tolist() if hasattr(train_losses, 'tolist') else list(train_losses)\n",
    "    val_losses = val_losses.tolist() if hasattr(val_losses, 'tolist') else list(val_losses)\n",
    "\n",
    "    return train_losses, val_losses, epoch, learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "500a344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, src_data, tgt_data, batch_size=32, pad_token_id=0):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test data.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average loss per non-pad token\n",
    "        accuracy: Percentage of correct predictions (excluding PADs)\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for src_batch, tgt_batch in create_batches(\n",
    "        src_data, tgt_data, batch_size=batch_size, pad_token=pad_token_id\n",
    "    ):\n",
    "        outputs = seq2seq.forward(\n",
    "            src_batch, tgt_batch,\n",
    "            teacher_forcing_ratio=0.0,\n",
    "            training=False\n",
    "        )\n",
    "\n",
    "        loss, _ = sequence_loss(outputs, tgt_batch, pad_token_id)\n",
    "        batch_tokens = np.sum(tgt_batch != pad_token_id)\n",
    "        total_loss += loss * batch_tokens\n",
    "        total_tokens += batch_tokens\n",
    "\n",
    "        # Accuracy calculation\n",
    "        pred_tokens = np.argmax(outputs, axis=2)  # [T, B]\n",
    "        correct = (pred_tokens == tgt_batch) & (tgt_batch != pad_token_id)\n",
    "        total_correct += np.sum(correct)\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    accuracy = total_correct / max(total_tokens, 1)\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "6724b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_model(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    path=\"model_checkpoint.npz\",\n",
    "    train_losses=None,\n",
    "    val_losses=None,\n",
    "    epoch=None,\n",
    "    learning_rate=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Save encoder and decoder parameters plus training metadata.\n",
    "    \"\"\"\n",
    "    save_data = {\n",
    "        \"encoder_embedding\": encoder.embedding,\n",
    "        \"decoder_embedding\": decoder.embedding,\n",
    "        \"decoder_fc_W\": decoder.fc_W,\n",
    "        \"decoder_fc_b\": decoder.fc_b,\n",
    "    }\n",
    "\n",
    "    for i, (W, b) in enumerate(encoder.rnn_params):\n",
    "        save_data[f\"encoder_W_{i}\"] = W\n",
    "        save_data[f\"encoder_b_{i}\"] = b\n",
    "\n",
    "    for i, (W, b) in enumerate(decoder.rnn_params):\n",
    "        save_data[f\"decoder_W_{i}\"] = W\n",
    "        save_data[f\"decoder_b_{i}\"] = b\n",
    "\n",
    "    if train_losses is not None:\n",
    "        save_data[\"train_losses\"] = np.array(train_losses)\n",
    "    if val_losses is not None:\n",
    "        save_data[\"val_losses\"] = np.array(val_losses)\n",
    "    if epoch is not None:\n",
    "        save_data[\"epoch\"] = epoch\n",
    "    if learning_rate is not None:\n",
    "        save_data[\"learning_rate\"] = learning_rate\n",
    "\n",
    "    np.savez(path, **save_data)\n",
    "    print(f\"‚úÖ Model saved to '{path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad24939",
   "metadata": {},
   "source": [
    "Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "id": "f7470e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_decay_fn(initial_lr, decay_rate, decay_steps):\n",
    "    \"\"\"\n",
    "    Returns a function that computes the decayed LR given the current step/epoch.\n",
    "    \n",
    "    Example:\n",
    "        lr_fn = get_lr_decay_fn(0.01, 0.95, 1)  # decay every epoch by 5%\n",
    "        new_lr = lr_fn(epoch)\n",
    "    \"\"\"\n",
    "    def lr_fn(step):\n",
    "        return initial_lr * (decay_rate ** (step / decay_steps))\n",
    "    return lr_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "f6034027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_with_rouge(model, test_src, test_tgt, batch_size, pad_idx=0):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    all_refs, all_preds = [], []\n",
    "    total_loss = 0\n",
    "    batches = list(create_batches(test_src, test_tgt, batch_size))\n",
    "\n",
    "    for src_batch, tgt_batch in batches:\n",
    "        outputs = model.forward(src_batch, tgt_batch, teacher_forcing_ratio=0.0)\n",
    "        trg_len, batch_size_ = tgt_batch.shape\n",
    "\n",
    "        # Calculate loss\n",
    "        batch_loss = 0\n",
    "        for t in range(1, trg_len):\n",
    "            logits = outputs[t]\n",
    "            targets = tgt_batch[t]\n",
    "           # Inside the for-loop over timesteps:\n",
    "            loss = masked_loss_single_step(logits, targets, pad_idx=pad_idx)\n",
    "            batch_loss += loss\n",
    "\n",
    "        total_loss += batch_loss / (trg_len - 1)\n",
    "\n",
    "        # Convert output predictions to string for ROUGE\n",
    "        preds = np.argmax(outputs, axis=2)  # [tgt_len, batch_size]\n",
    "        for i in range(preds.shape[1]):\n",
    "            pred_seq = preds[:, i]\n",
    "            tgt_seq = tgt_batch[:, i]\n",
    "\n",
    "            pred_str = ' '.join([str(tok) for tok in pred_seq if tok != pad_idx])\n",
    "            tgt_str = ' '.join([str(tok) for tok in tgt_seq if tok != pad_idx])\n",
    "\n",
    "            all_preds.append(pred_str)\n",
    "            all_refs.append(tgt_str)\n",
    "\n",
    "    # ROUGE calculation\n",
    "    rouge1, rouge2, rougeL = [], [], []\n",
    "    for ref, hyp in zip(all_refs, all_preds):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        rouge1.append(scores['rouge1'].fmeasure)\n",
    "        rouge2.append(scores['rouge2'].fmeasure)\n",
    "        rougeL.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    avg_rouge1 = np.mean(rouge1)\n",
    "    avg_rouge2 = np.mean(rouge2)\n",
    "    avg_rougeL = np.mean(rougeL)\n",
    "    avg_test_loss = total_loss / len(batches)\n",
    "\n",
    "    print(f\"\\nüß™ Final Evaluation:\")\n",
    "    print(f\"Test Loss     : {avg_test_loss:.4f}\")\n",
    "    print(f\"ROUGE-1 Score : {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-2 Score : {avg_rouge2:.4f}\")\n",
    "    print(f\"ROUGE-L Score : {avg_rougeL:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "b9a36032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, base_lr=0.005, decay=0.98):\n",
    "    # Smooth exponential decay per epoch (you can try cosine annealing here too)\n",
    "    return base_lr * (decay ** epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "8dfd0cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def debug_train_step(seq2seq, src_batch, tgt_batch, learning_rate=0.01,\\n                     teacher_forcing_ratio=1.0, pad_idx=0, chunk_size=10):\\n\\n    print(\"\\nüß™ DEBUG TRAIN STEP\")\\n    print(f\"original src_batch shape: {src_batch.shape}\")   # (64, 500)\\n    print(f\"original tgt_batch shape: {tgt_batch.shape}\")   # (64, 50)\\n\\n    # ‚úÖ TRANSPOSE the input: shape (seq_len, batch_size)\\n    src_batch = src_batch.T\\n    tgt_batch = tgt_batch.T\\n\\n    print(f\"transposed src_batch shape: {src_batch.shape}\")   # (500, 64)\\n    print(f\"transposed tgt_batch shape: {tgt_batch.shape}\")   # (50, 64)\\n\\n    try:\\n        outputs = seq2seq.forward(\\n            src_batch, tgt_batch,\\n            teacher_forcing_ratio=teacher_forcing_ratio,\\n            training=True\\n        )\\n        print(f\"‚úÖ outputs shape: {outputs.shape}\")\\n    except Exception as e:\\n        print(f\"‚ùå Error during forward pass: {e}\")\\n        raise e\\n\\n    return 0.0'"
      ]
     },
     "execution_count": 1193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def debug_train_step(seq2seq, src_batch, tgt_batch, learning_rate=0.01,\n",
    "                     teacher_forcing_ratio=1.0, pad_idx=0, chunk_size=10):\n",
    "\n",
    "    print(\"\\nüß™ DEBUG TRAIN STEP\")\n",
    "    print(f\"original src_batch shape: {src_batch.shape}\")   # (64, 500)\n",
    "    print(f\"original tgt_batch shape: {tgt_batch.shape}\")   # (64, 50)\n",
    "\n",
    "    # ‚úÖ TRANSPOSE the input: shape (seq_len, batch_size)\n",
    "    src_batch = src_batch.T\n",
    "    tgt_batch = tgt_batch.T\n",
    "\n",
    "    print(f\"transposed src_batch shape: {src_batch.shape}\")   # (500, 64)\n",
    "    print(f\"transposed tgt_batch shape: {tgt_batch.shape}\")   # (50, 64)\n",
    "\n",
    "    try:\n",
    "        outputs = seq2seq.forward(\n",
    "            src_batch, tgt_batch,\n",
    "            teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "            training=True\n",
    "        )\n",
    "        print(f\"‚úÖ outputs shape: {outputs.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during forward pass: {e}\")\n",
    "        raise e\n",
    "\n",
    "    return 0.0\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "c0d72591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded from 'model_checkpoint.npz'\n",
      "‚úÖ Model loaded from 'model_checkpoint.npz'\n",
      "Training Started\n",
      "üîç Probs sample: [3.8019887e-06 3.7565269e-06 4.5492006e-06 4.3967473e-03 1.7282981e-04\n",
      " 1.8967474e-02 9.7548596e-05 2.9814336e-04 4.8606787e-04 5.9206326e-02] sum: 1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1194], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# 8Ô∏è‚É£ Start Training\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 175\u001b[0m     train_losses, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq2seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_tgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_teacher_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_teacher_forcing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_teacher_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_teacher_forcing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPAD_IDX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1194], line 116\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(seq2seq, train_src, train_tgt, val_src, val_tgt, n_epochs, batch_size, learning_rate, patience, initial_teacher_forcing, final_teacher_forcing, pad_idx, checkpoint_path)\u001b[0m\n\u001b[0;32m    113\u001b[0m src_batch \u001b[38;5;241m=\u001b[39m src_batch\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    114\u001b[0m tgt_batch \u001b[38;5;241m=\u001b[39m tgt_batch\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 116\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq2seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_idx\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m    126\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[1186], line 55\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(seq2seq, src_batch, tgt_batch, learning_rate, teacher_forcing_ratio, pad_idx, chunk_size)\u001b[0m\n\u001b[0;32m     52\u001b[0m encoder_grads \u001b[38;5;241m=\u001b[39m seq2seq\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mbackward(decoder_grads)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# ‚úÖ Clip grads\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_grads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_grads\u001b[49m\n\u001b[0;32m     56\u001b[0m clipped_grads \u001b[38;5;241m=\u001b[39m clip_gradients_global(all_grads)\n\u001b[0;32m     57\u001b[0m seq2seq\u001b[38;5;241m.\u001b[39mapply_gradients(clipped_grads, learning_rate)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Seed everything for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Load Data\n",
    "train_src = np.load(\"../RNN_PREPROCESSED/train_src.npy\", allow_pickle=True)\n",
    "train_tgt = np.load(\"../RNN_PREPROCESSED/train_tgt.npy\", allow_pickle=True)\n",
    "val_src   = np.load(\"../RNN_PREPROCESSED/val_src.npy\", allow_pickle=True)\n",
    "val_tgt   = np.load(\"../RNN_PREPROCESSED/val_tgt.npy\", allow_pickle=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Hyperparameters\n",
    "INPUT_DIM = np.max(train_src) + 1\n",
    "OUTPUT_DIM = np.max(train_tgt) + 1\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.2\n",
    "PAD_IDX = 0\n",
    "\n",
    "initial_teacher_forcing = 1.0\n",
    "final_teacher_forcing = 0.5\n",
    "batch_size = 64\n",
    "extra_epochs = 30\n",
    "initial_lr = 0.005\n",
    "checkpoint_path = \"model_checkpoint.npz\"\n",
    "\n",
    "# -----------------------------\n",
    "# 4Ô∏è‚É£ Model Init\n",
    "encoder = RNNEncoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = RNNDecoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "\n",
    "# -----------------------------\n",
    "# 5Ô∏è‚É£ Try Load Checkpoint\n",
    "start_epoch = 0\n",
    "train_losses, val_losses = [], []\n",
    "learning_rate = initial_lr\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    try:\n",
    "        train_losses, val_losses, start_epoch, learning_rate = load_model(\n",
    "            encoder, decoder, path=checkpoint_path\n",
    "        )\n",
    "        seq2seq = Seq2Seq(encoder, decoder)  # Re-wrap after loading\n",
    "        print(f\"‚úÖ Model loaded from '{checkpoint_path}'\")\n",
    "    except ValueError:\n",
    "        os.remove(checkpoint_path)\n",
    "        start_epoch = 0\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ LR Scheduler\n",
    "def lr_schedule(epoch, base_lr=initial_lr, decay=0.98):\n",
    "    warmup_epochs = 3\n",
    "    if epoch < warmup_epochs:\n",
    "        return base_lr * (epoch + 1) / warmup_epochs\n",
    "    return base_lr * (decay ** (epoch - warmup_epochs))\n",
    "\n",
    "# -----------------------------\n",
    "# 7Ô∏è‚É£ Train Model Function\n",
    "def train_model(\n",
    "    seq2seq,\n",
    "    train_src, train_tgt,\n",
    "    val_src, val_tgt,\n",
    "    n_epochs=30,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    patience=5,\n",
    "    initial_teacher_forcing=1.0,\n",
    "    final_teacher_forcing=0.5,\n",
    "    pad_idx=0,\n",
    "    checkpoint_path=None\n",
    "):\n",
    "    print(\"Training Started\", flush=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    current_start_epoch = start_epoch\n",
    "\n",
    "    for epoch in range(current_start_epoch, current_start_epoch + n_epochs):\n",
    "        start_time = time.time()\n",
    "        lr = lr_schedule(epoch, base_lr=learning_rate)\n",
    "        teacher_forcing_ratio = max(final_teacher_forcing, initial_teacher_forcing * (0.98 ** epoch))\n",
    "\n",
    "        # Shuffle data\n",
    "        shuffled_indices = np.random.permutation(len(train_src))\n",
    "        train_src_shuffled = train_src[shuffled_indices]\n",
    "        train_tgt_shuffled = train_tgt[shuffled_indices]\n",
    "\n",
    "        # Create batches\n",
    "        batches = create_length_sorted_batches(\n",
    "            train_src_shuffled,\n",
    "            train_tgt_shuffled,\n",
    "            batch_size=batch_size,\n",
    "            pad_token=pad_idx\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for src_batch, tgt_batch in batches:\n",
    "            src_batch = src_batch.T\n",
    "            tgt_batch = tgt_batch.T\n",
    "\n",
    "            batch_loss = train_step(\n",
    "                seq2seq,\n",
    "                src_batch,\n",
    "                tgt_batch,\n",
    "                learning_rate=lr,\n",
    "                teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "                pad_idx=pad_idx\n",
    "            )\n",
    "\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = total_loss / max(num_batches, 1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(\n",
    "            seq2seq, val_src, val_tgt,\n",
    "            batch_size=batch_size,\n",
    "            pad_token_id=pad_idx\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"üìä Epoch {epoch + 1:02}/{current_start_epoch + n_epochs} | \"\n",
    "              f\"LR: {lr:.6f} | TF: {teacher_forcing_ratio:.2f} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2%} | ‚è±Ô∏è {duration:.2f}s\")\n",
    "\n",
    "        if checkpoint_path is not None and val_loss < best_val_loss:\n",
    "            save_model(\n",
    "                seq2seq.encoder,\n",
    "                seq2seq.decoder,\n",
    "                path=checkpoint_path,\n",
    "                train_losses=train_losses,\n",
    "                val_losses=val_losses,\n",
    "                epoch=epoch + 1,\n",
    "                learning_rate=lr\n",
    "            )\n",
    "            print(\"‚úÖ Checkpoint saved.\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"üõë Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n‚úÖ Training complete.\")\n",
    "    print(f\"Recent train losses: {train_losses[-5:]}\")\n",
    "    print(f\"Recent val losses:   {val_losses[-5:]}\")\n",
    "    print(f\"Recent val accs:     {[f'{acc:.2%}' for acc in val_accuracies[-5:]]}\")\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# -----------------------------\n",
    "# 8Ô∏è‚É£ Start Training\n",
    "if __name__ == \"__main__\":\n",
    "    train_losses, val_losses, val_accuracies = train_model(\n",
    "        seq2seq,\n",
    "        train_src, train_tgt,\n",
    "        val_src, val_tgt,\n",
    "        n_epochs=extra_epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        initial_teacher_forcing=initial_teacher_forcing,\n",
    "        final_teacher_forcing=final_teacher_forcing,\n",
    "        pad_idx=PAD_IDX,\n",
    "        checkpoint_path=checkpoint_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = np.load(\"../RNN_PREPROCESSED/test_src.npy\")\n",
    "test_tgt = np.load(\"../RNN_PREPROCESSED/test_tgt.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c93327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Final Evaluation:\n",
      "Test Loss     : 7.8216\n",
      "ROUGE-1 Score : 0.0349\n",
      "ROUGE-2 Score : 0.0000\n",
      "ROUGE-L Score : 0.0349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_with_rouge(seq2seq, test_src, test_tgt, batch_size=32, pad_idx=PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1010], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Plot Accuracy (if added)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mval_accuracies\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy Over Epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy (if added)\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notesyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
